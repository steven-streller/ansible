# üõ† SRE & DevOps Glossar

## 1Ô∏è‚É£ Incident Management

| Begriff | Erkl√§rung |
|---------|-----------|
| Incident | Ein unvorhergesehenes Ereignis, das den normalen Betrieb eines Services unterbricht |
| Severity / Sev | Klassifizierung eines Incidents nach Auswirkung auf Service (z.B. Sev1 = kritischer Ausfall) |
| War Room | Virtueller oder physischer Raum, in dem ein Incident koordiniert und gel√∂st wird |
| Pager / On-Call | Bereitschaftsdienst f√ºr kritische Systeme, der bei Alerts alarmiert wird |
| Escalation | Weiterleitung eines Incidents an h√∂here Support- oder F√ºhrungsebene |
| Postmortem / Retrospektive | Analyse nach Incident: Ursachen, Auswirkungen, Ma√ünahmen |
| Root Cause Analysis (RCA) | Untersuchung der Hauptursache eines Problems, um Wiederholung zu verhindern |
| Mitigation | Sofortige Ma√ünahme zur Reduzierung der Auswirkungen eines Incidents |
| Recovery | Ma√ünahmen zur Wiederherstellung des normalen Servicebetriebs |
| Runbook | Dokumentierte Schritt-f√ºr-Schritt-Anleitungen zur L√∂sung bekannter Incidents |

## 2Ô∏è‚É£ SRE-Kernkonzepte

| Begriff | Erkl√§rung |
|---------|-----------|
| SLO (Service Level Objective) | Konkretes Ziel f√ºr Verf√ºgbarkeit, Latenz, Durchsatz oder Fehlerquote eines Service |
| SLA (Service Level Agreement) | Vertraglich vereinbarte Garantien f√ºr Servicequalit√§t gegen√ºber Kunden |
| SLI (Service Level Indicator) | Messbare Kennzahl zur √úberwachung eines SLO (z.B. 99,9% Requests erfolgreich) |
| Error Budget | Maximal tolerierbarer Anteil von Fehlern oder Ausfallzeit f√ºr einen Service |
| Blameless Culture | Fokus auf Lernen und Verbesserung statt Schuldzuweisung bei Incidents |
| Post-Incident Review | Analyse nach einem Vorfall, Lessons Learned, Dokumentation von Ma√ünahmen |

## 3Ô∏è‚É£ DevOps / Deployment

| Begriff | Erkl√§rung |
|---------|-----------|
| CI/CD | Continuous Integration / Continuous Deployment: Automatisierter Build-Test-Deploy-Prozess |
| Canary Deployment | Schrittweises Ausrollen neuer Versionen auf kleine Nutzergruppe |
| Blue-Green Deployment | Parallelbetrieb alter & neuer Version, nahtloser Wechsel |
| Rollback | Zur√ºcksetzen auf vorherige stabile Version |
| Feature Flag | Neue Features tempor√§r ein-/ausschalten ohne Deployment |
| Infrastructure as Code (IaC) | Infrastruktur wird wie Software definiert und versioniert (z.B. Terraform, Ansible) |
| GitOps | CI/CD + IaC: √Ñnderungen an Infrastruktur via Git-Pull Requests |

## 4Ô∏è‚É£ Monitoring & Observability

| Begriff | Erkl√§rung |
|---------|-----------|
| Monitoring | √úberwachung von Systemmetriken (CPU, RAM, Disk, Netzwerk) |
| Alerting | Automatische Benachrichtigung bei √úberschreitung definierter Schwellenwerte |
| Observability | F√§higkeit, Zustand und Verhalten eines Systems aus Logs, Metriken, Traces zu verstehen |
| Metrics / Metriken | Messbare Werte wie Response Time, Error Rate, CPU Usage |
| Logs | Ereignisprotokolle von Systemen oder Anwendungen |
| Tracing | Verfolgung des Pfads einzelner Requests durch Systeme (z.B. Jaeger, OpenTelemetry) |
| Dashboard | Visualisierung von Metriken & Logs (z.B. Grafana) |
| Health Check | Pr√ºft, ob Service korrekt l√§uft und Anfragen bedienen kann |

## 5Ô∏è‚É£ SRE Operations / Tools

| Begriff | Erkl√§rung |
|---------|-----------|
| War Room | Zentraler Ort zur Koordination kritischer Incidents |
| Incident Commander | Person, die Incident aktiv steuert und Entscheidungen trifft |
| Postmortem Template | Standardisierte Struktur f√ºr Incident-Analyse |
| Runbook Automation | Automatisierte Schritte aus Runbooks ausf√ºhren (z.B. Scripts f√ºr Recovery) |
| Chaos Engineering | Geplante St√∂rungen zur √úberpr√ºfung der Resilienz |
| Load Balancer | Verteilt Traffic auf mehrere Instanzen f√ºr High Availability |
| Auto-Scaling | Dynamische Anpassung von Ressourcen bei Last√§nderungen |
| Redundancy / HA | Mehrfach vorhandene Systeme f√ºr Ausfallsicherheit |
| Backup / DR | Sicherungen + Disaster Recovery-Strategie |

## 6Ô∏è‚É£ Sonstiges / Begriffe im SRE-Alltag

| Begriff | Erkl√§rung |
|---------|-----------|
| KPI | Key Performance Indicator ‚Äì Leistungskennzahl eines Services |
| MTTR | Mean Time To Repair ‚Äì durchschnittliche Wiederherstellungszeit nach Ausfall |
| MTTF | Mean Time To Failure ‚Äì durchschnittliche Zeit bis zum Ausfall eines Systems |
| MTBF | Mean Time Between Failures ‚Äì durchschnittliche Zeit zwischen Ausf√§llen |
| SLA Breach | Versto√ü gegen Service Level Agreement |
| Change Management | Planen, Testen, Dokumentieren und Freigeben von √Ñnderungen an Produktionssystemen |
| Alert Fatigue | Ersch√∂pfung durch zu viele Benachrichtigungen / false positives |
| PagerDuty / OpsGenie | Tools zur Alarmierung & On-Call-Verwaltung |


# üö® SRE Incident Handling Cheat Sheet

## 1Ô∏è‚É£ Incident Lifecycle

| Schritt | Beschreibung |
|---------|--------------|
| Detection / Alert | Ein Monitoring-Tool oder Nutzer meldet einen Incident. Alerts k√∂nnen via PagerDuty, OpsGenie, Slack etc. kommen. |
| Classification / Severity | Incident wird nach Auswirkung klassifiziert (Sev1 = kritisch, Sev2 = hoch, Sev3 = mittel, Sev4 = niedrig). |
| Notification / Escalation | Benachrichtigung des On-Call Engineers oder Incident Commander. Eskalation bei Sev1/Sev2. |
| War Room | Virtueller oder physischer Raum, in dem Incident koordiniert wird (alle Beteiligten zusammen). |
| Mitigation | Sofortma√ünahmen, um die Auswirkungen zu reduzieren (z.B. Traffic auf Backup umleiten, fehlerhafte Instanz stoppen, Hotfix einspielen). |
| Investigation / Root Cause Analysis (RCA) | Ursachenanalyse: Logs pr√ºfen, Monitoring-Daten auswerten, Konfiguration und Infrastruktur analysieren. Ziel: Hauptursache finden. |
| Resolution / Recovery | Service wiederherstellen. Schritte k√∂nnen Restart, Rollback, Scaling, Patch Deployment sein. |
| Verification / Validation | √úberpr√ºfen, dass Service stabil l√§uft und Fehler behoben sind. Health Checks, Monitoring, Test Requests. |
| Post-Incident Review / Postmortem | Dokumentation des Incidents: Timeline, Ursachen, getroffene Ma√ünahmen, Lessons Learned. |

---

## 2Ô∏è‚É£ Typische Begriffe & Aktionen

| Begriff | Erkl√§rung |
|---------|-----------|
| Incident Commander | Verantwortlich f√ºr Steuerung des Incidents, Entscheidungen treffen, Kommunikation koordinieren |
| Mitigation | Sofortma√ünahmen zur Minimierung der Auswirkungen eines Incidents |
| Resolution | Endg√ºltige Behebung des Problems, sodass Service wieder normal l√§uft |
| Root Cause / RCA | Prim√§re Ursache des Problems, die langfristig behoben werden muss |
| Runbook | Schritt-f√ºr-Schritt-Anleitungen f√ºr bekannte Incidents oder Standardma√ünahmen |
| Blameless Postmortem | Analyse ohne Schuldzuweisung, Fokus auf Lernen und Verbesserung |
| Change / Deployment | Eventuell notwendige Konfigurations√§nderungen oder Software-Updates zur Behebung |
| Communication | Updates an Stakeholder, Kunden oder interne Teams w√§hrend des Incidents |
| Recovery Point | Zeitpunkt, auf den man Daten oder Systeme wiederherstellt (z.B. Backup) |
| Recovery Time | Dauer, bis der Service wieder verf√ºgbar ist (MTTR) |

---

## 3Ô∏è‚É£ Incident Workflow (Schnell√ºbersicht)

1. **Alert erhalten** ‚Üí Pr√ºfen der Severity  
2. **War Room √∂ffnen** ‚Üí On-Call + Incident Commander koordinieren  
3. **Mitigation starten** ‚Üí Sofortma√ünahmen zur Stabilisierung  
4. **Investigation** ‚Üí Logs, Metriken, Traces pr√ºfen  
5. **Resolution** ‚Üí Hotfix, Rollback, Reboot, Scaling, Config-Change  
6. **Verify** ‚Üí Health Checks, Monitoring-Alerts pr√ºfen  
7. **Postmortem** ‚Üí Ursachen, Timeline, Lessons Learned dokumentieren  
8. **Preventive Measures** ‚Üí Runbook aktualisieren, Monitoring/Alerts anpassen  

---

## 4Ô∏è‚É£ Praktische Tipps f√ºr SRE

- **Severity richtig setzen** ‚Üí wirkt auf Eskalation, Priorisierung, Ressourcen.  
- **Kommunikation ist Key** ‚Üí Stakeholder immer up-to-date halten.  
- **Mitigation zuerst, Root Cause danach** ‚Üí Stoppe Impact, bevor du Ursachen analysierst.  
- **Monitoring & Alerts** ‚Üí Nutze Grafana/Prometheus oder ELK, um schnell Trends und Anomalien zu erkennen.  
- **Blameless Culture** ‚Üí Fokus auf Prozesse, nicht auf Personen.  
- **Lessons Learned** ‚Üí Immer Runbooks aktualisieren, so dass gleiche Incidents schneller gel√∂st werden k√∂nnen.  
